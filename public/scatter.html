<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>

    <!-- Favicon -->
    <link rel="icon" href="img/logo/logo1.svg" type="image/x-icon" />

    <style>
        /* Apply smooth scrolling behavior */
        /* .smooth-scroll {
            scroll-behavior: smooth;
        } */

        [x-cloak] {
            display: none !important;
        }

        .font-roboto {
            font-family: 'Roboto', sans-serif;
        }

        .font-merri {
            font-family: 'Merriweather', serif;
        }

        .font-pop {
            font-family: 'Poppins', sans-serif;
        }

        .font-lora {
            font-family: 'Lora', sans-serif;
        }

        .font-ledger {
            font-family: 'Ledger', sans-serif;
        }

        .font-signika {
            font-family: 'Signika', sans-serif;
        }
    </style>

    <!-- Fonts -->
    <link rel="stylesheet" href="../font.css">

    <!-- css  -->
    <link rel="stylesheet" href="../style.css">

    <!-- mathjax  -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            messageStyle: "normal",
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                processEscapes: true
                }
            });
      </script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>



    <title>Scatter to Regression</title>

</head>

<body class="font-pop bg-white text-gray-700 dark:bg-gray-900 dark:text-gray-200">

    <!-- navbar -->
    <nav x-data="{ isOpen: false }" class="relative ">
        <div class="container mx-auto py-4 md:mx-20 px-6 md:px-0 md:py-0 md:flex md:justify-between md:items-center">
            <div class="flex items-center justify-between">

                <!-- logo + name  -->
                <div class="flex items-center">
                    <a class="mr-auto" href="index.html">
                        <img id="logo" class="w-full h-8 pe-6 sm:h-8 fill-current dark:filter-white"
                            src="./img/logo/logo5.svg" alt="">
                    </a>
                    <a class="hidden md:inline-block text-lg font-merri font-semibold tracking-widest uppercase rounded-lg focus:outline-none focus:shadow-outline ml-2"
                        href="index.html">Marco Zausch</a>
                </div>

                <!-- Mobile menu button -->
                <div class="flex lg:hidden">
                    <button x-cloak @click="isOpen = !isOpen" type="button"
                        class="hover:text-gray-600 dark:hover:text-gray-400 focus:outline-none"
                        aria-label="toggle menu">
                        <svg x-show="!isOpen" xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none"
                            viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M4 8h16M4 16h16" />
                        </svg>

                        <svg x-show="isOpen" xmlns="http://www.w3.org/2000/svg" class="w-6 h-6" fill="none"
                            viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                            <path stroke-linecap="round" stroke-linejoin="round" d="M6 18L18 6M6 6l12 12" />
                        </svg>
                    </button>
                </div>
            </div>

            <!-- Mobile Menu open: "block", Menu closed: "hidden" -->
            <div x-cloak :class="[isOpen ? 'translate-x-0 opacity-100 ' : 'opacity-0 -translate-x-full']"
                class="absolute inset-x-0 z-20 w-full px-6 py-12 transition-all duration-300 ease-in-out bg-white dark:bg-gray-900 md:mt-0 md:px-0 md:top-0 md:relative md:bg-transparent md:w-auto md:opacity-100 md:translate-x-0 md:flex md:items-center">
                <div class="flex flex-col md:flex-row md:mx-0">
                    <a class="px-4 py-0 pt-1 text-lg  text-slate-700 dark:text-gray-400 hover:text-fuchsia-600 dark:hover:text-sky-300"
                        href="#">Blog</a>
                    <a class="px-4 py-0 pt-1 text-lg  text-slate-700 dark:text-gray-400 hover:text-fuchsia-600 dark:hover:text-sky-300"
                        href="#">Portfolio</a>
                    <a class="px-4 py-0 pt-1 text-lg  text-slate-700 dark:text-gray-400 hover:text-fuchsia-600 dark:hover:text-sky-300"
                        href="#">About</a>
                    <a class="px-4 py-0 pt-1 text-lg text-slate-700 dark:text-gray-400 hover:text-fuchsia-600 dark:hover:text-sky-300"
                        href="#">Contact</a>
                </div>
            </div>
        </div>
    </nav>

    <aside
        class="hidden md:block right-0 w-1/6 py-8 fixed top-1/2 transform -translate-y-1/2 flex-col items-center overflow-y-auto bg-white dark:bg-gray-900 dark:border-gray-700">
        <nav class="flex flex-col flex-1 space-y-8 items-center justify-center">
            <a type="button" id="chevron-up">
                <span
                    class="icon-[heroicons-outline--chevron-up] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
            </a>

            <a type="button" id="chevron-down">
                <span
                    class="icon-[heroicons-outline--chevron-down] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
            </a>

            <a class=""></a>

            <a id="font-select" data-tooltip-placement="left">
                <span
                    class="icon-[fa6-solid--font] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
            </a>

            <a class=""></a>

            <a type="button" href="#top" id="chevron-up-double">
                <span
                    class="icon-[heroicons-outline--chevron-double-up] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
            </a>

            <a type="button" href="#footer" id="chevron-down-double">
                <span
                    class="icon-[heroicons-outline--chevron-double-down] w-8 h-8 fill-current text-gray-500 dark:text-gray-500 hover:text-blue-600 dark:hover:text-blue-400 ms-5"></span>
            </a>

    </aside>

    <!-- Main content -->
    <main class="flex justify-center items-center min-h-screen pt-20">
        <article id="article"
            class="w-5/6 px-1 prose-sm md:w-1/2 max-w-full prose md:prose-xl prose-stone text-justify leading-10 dark:prose-invert">

            <!-- title  -->
            <h1 class="text-left leading-snug">From Scatterplot to Regression Line</h1>

            <!-- intro -->
            <section>
                <!-- table of contents  -->
                <div class="float-none md:float-right">
                    <div class="border rounded-lg w-fit pl-4 pr-4 pb-0 pt-0 ml-6 mt-2 mb-1 shadow-sm z-40">
                        <p class="text-lg font-bold text-left border-b border-gray-300">Contents</p>
                        <ul class="text-base list-none text-left pl-0 ">
                            <li><a href="#intro">Introduction</a>
                            <li><a href="#terminology">Terminology</a>
                            <li><a href="#scatterplot">The Scatterplot</a>
                            <li><a href="#spread">Spread</a>
                            <li><a href="#averages">Averages</a>
                            <li><a href="#goa">Point &amp; Graph of Averages</a>
                            <li><a href="#sdline">Standard Deviation Line</a>
                            <li><a href="#correlation">Pearson's Correlation</a>
                            <li><a href="#regression_method">Regression Method</a>
                            <li><a href="#OLS">Method of Least Squares</a>
                            <li><a href="#interpretation">Interpretation</a>
                            <li><a href="#summary">Summary</a>
                            <li><a href="#sources">Sources</a>
                        </ul>
                    </div>
                </div>
                <h2 id="intro">Intro</h2>
                <p>This text explains simple linear regression analysis for those with a basic understanding of
                    statistics. We will develop a regression model step-by-step and learn about its components. </p>
                <p>Simple linear regression is an excellent introduction to the topic of “statistical inference”. While
                    there are other methods of doing statistical inference, looking at regression allows us to discuss
                    some key concepts of statistics and models that make predictions in general. What is discussed here
                    acts as a stepping stone to more advanced regression and other inference methods.</p>
                <p>Techniques of inference or prediction have become a noticeable part of modern life and it is
                    useful to have some knowledge of them. However, discussing these topics requires specific
                    vocabulary, which I will introduce as we go along. Before we begin, however, some ambiguities
                    and terms should be clarified.</p>
            </section>

            <!-- terminology  -->
            <section>
                <h2 id="terminology">Terminology</h2>
                <p>Mathematics is a language in itself, it maybe precise but not always clear. Different authors use
                    different symbols, the same thing can have many synonyms, or vice versa, one word can have
                    different meanings. I'll do my best to keep the math to a minimum and extract the essentials,
                    otherwise I'll choose the most commonly used symbols, clarify any synonyms/homonyms and such.
                </p>
                <p>Here is the basic vocabulary we need.</p>
                <p>Regression is a branch of <strong>statistical inference</strong>, which refers to the process of
                    drawing conclusions about a population based on sample of that population. Linear regression is
                    a type of “<strong>frequentist inference</strong>” method that focuses on the frequency or
                    probability of observed events. However, Bayesian or Likelihood Inference and Bootstrap methods
                    exist too. </p>
                <p>I also mentioned <strong>predictions</strong> which are sometimes used interchangeably with
                    <strong>forecasts</strong> but do have different meanings in certain fields.
                    From what I could <a
                        href="https://stats.stackexchange.com/questions/65287/difference-between-forecast-and-prediction">gather</a><i
                        class="icon-[ei--external-link] w-6 h-6 -m-0.5" aria-hidden="true"></i>,
                    forecasting involves predicting future events, often using time-series analysis.
                    This assumes that forecasting is a type of prediction that deals with future events, while simple
                    “prediction” is limited to the sample data and does not extend to the future. In demography and
                    climate science the term <strong>projection</strong>, refers to a forecast based on a specific
                    scenario.
                </p>
                <p>I will use the term prediction throughout the text since we are not working with time. However, I
                    will not be so strict as to limit a "prediction" to the sample data. I still consider an estimate
                    beyond the range of the sample data to be a prediction. Although it is advisable not to stray too
                    far from the available data, the prediction may no longer make sense, and we'll see why.</p>
                <p><strong>Simple regression</strong> describes the relationship between an <em>independent</em>
                    (explanatory) variable and a single <em>dependent variable</em> (response). The relationship is
                    often assumed to be linear, and the equation of a <em>simple linear regression</em> is:
                    $$y=β_0+β_1x+ε$$<br />
                    Where: <br />
                    $y=$ response, the quantity to be estimated or modeled.<br />
                    $x=$ explanatory variable, the known input to the model, used as a predictor of $y$.<br />
                    $b_0$ (beta zero) $=$ $y$-<strong>intercept</strong> of the line, that is, the point at which
                    the line <br />
                    intercepts or cuts through the $y$-axis<br />
                    $b_1$ (beta one) = <strong>Slope</strong> of the line, i.e. the change (amount of increase or
                    <br />
                    decrease) of $y$ for every 1-unit increase in $x$.<br />
                    $\varepsilon=$ Random error component, adds an allowance for error in the prediction.
                </p>
                <p>By the end of this text, we'll arrive at an equation that has little hats on some symbols,
                    indicating that the symbols below them are <strong>estimates</strong> of the true but unknown
                    parameters.<br />
                    $$\hat{y}=\hat\beta_0+\hat\beta_{1}x$$In this equation, the error term $ε$ is missing, the reason
                    being that in the process of finding an estimate of the first equation, we are specifically
                    minimizing the errors, where the sum of errors becomes zero $\sum ε_i =0$, the uncertainty in the
                    predictions is still represented by the hats, telling us that we're dealing with estimates. </p>
                <p><strong>Linear regression</strong> extends simple regression to include multiple explanatory
                    variables and aims to estimate the linear relationship between these predictors and a single
                    response.<br />
                    $$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_kx_k + ε$$</p>
                <p>There are many synonyms for the independent and dependent variables. I have introduced you to
                    <strong>explanatory and response variable</strong>, which I find more expressive because the
                    explanatory variable explains how the other variable responds. Also, e$x$planatory has the
                    symbol $x$ in it, which denotes that variable in the formula.
                </p>
                <p>The explanatory and response variables are <strong>characteristics</strong> of
                    <strong>observational units</strong> (person, thing, transaction, or event), that we measured,
                    observed or polled from a <strong>sample</strong> (subset) of a <strong>population</strong>
                    (entirety of observational units of interest).
                </p>
                <p>In statistics we're almost always dealing with a sample because we rarely get information from
                    the entire population. <strong>Inferring</strong> something from the known (sample) about the
                    unknown (the rest of the population) is the main reason for doing statistics in the first place.
                </p>
                <p>Since we’re mostly dealing with samples I will use the <strong>sample “statistics”</strong>
                    instead of <strong>population “parameters”</strong>. Sample statistics are approximations of the
                    corresponding population parameters. Often they’re computed by dividing them by $(n-1)$ instead
                    of just $n$. This so-called <strong>Bessel's correction</strong>, is done to give the
                    approximated population parameter more variability than our sample might reflect. We are being
                    conservative here and saying that what we measured is probably biased, and the whole population
                    certainly shows more variability than we see in our sample</p>
                <p>The <strong>direction of the dependency</strong> is not always clear, e.g. we could try to
                    explain success as an outcome of a personality trait, but the reverse could also be
                    investigated. In such a case we have to decide what makes more sense, what effect we want to
                    study, what is influencing and what is being influenced.</p>
                <p><strong>Sampling aka. data collection</strong> is of utmost importance for statistics, most of
                    what’s wrong with statistics comes from error-prone data collection.<br />
                    I will shamefully shorten the topic for now and just say that each subgroup (e.g. population
                    strata, generation etc.) of the population should have an equal chance of being included in the
                    sample and the sample should be a good representation of the larger population. This is called a
                    <strong>simple random sample</strong>. Here, the requirement for a representative sample is met
                    by randomly selecting statistical units, often assisted by a random number generator, to avoid
                    introducing <strong>selection bias</strong>. Examples of selection bias include:
                </p>
                <ul>
                    <li>Surveying only Western college students for <a
                            href="https://www.scientificamerican.com/podcast/episode/psychology-studies-biased-toward-we-10-08-07/">psychological
                            studies</a><i class="icon-[ei--external-link] w-6 h-6 -m-0.5" aria-hidden="true"></i> and
                        attributing the findings to the broader population or even all of
                        humanity. Also called <em>volunteer bias</em>.</li>
                    <li><em>Publication bias</em>, where academic research is only published if significant findings
                        are made. Shifting publications in favor of positive results.</li>
                    <li>Or when the media neglects journalistic standards of neutrality and objectivity, in favor of
                        certain worldviews and cultures, or worse, <a
                            href="https://en.wikipedia.org/wiki/Manufacturing_Consent">pandering to particular
                            ideologies</a><i class="icon-[ei--external-link] w-6 h-6 -m-0.5" aria-hidden="true"></i>,
                        aka <em>media bias</em>. </li>
                </ul>
                <p>Terminology can get messy when we talk about machine learning, as old concepts get new names to
                    better fit the context. Here are the main vocabulary shifts between statistics and machine
                    learning. </p>
                <ul>
                    <li>A sample becomes a <strong>dataset</strong>.</li>
                    <li>Explanatory (independent) variables ($x$) become <strong>features</strong>. </li>
                    <li>Predictor (dependent) variable ($y$) becomes <strong>target variable</strong> or
                        <strong>label</strong>.
                    </li>
                </ul>
                <p><strong>After we have built a model</strong> and put it into practice, we feed an
                    <strong>input</strong> $x$ to the model and expect an <strong>output</strong> – an estimate of $y$ –
                    denoted by
                    $\hat{y}$.
                </p>
                <p>To evaluate the quality of the model, we can compare the estimate $\hat{y}$ with the <strong>true
                        label</strong> $y$. The difference between the true label and the output is called the
                    <strong>error</strong>.
                </p>
                <p>$$error=(y-\hat{y})$$</p>
                <p>Similar but not to be confused with the errors are <strong>deviations</strong>. A deviation is
                    the difference between an observed value and a measure of central tendency (such as the mean or
                    median). Deviations are typically used to understand the <strong>variability</strong> within a
                    dataset.</p>
                <p>$$deviation=(y-\bar{y})$$ Now we’re good to go.</p>
            </section>

            <!-- scatterplot  -->
            <section>
                <h2 id="scatterplot">The Scatterplot</h2>
                <p>In the beginning there was the scatterplot. When Sir Francis Galton (1822 - 1911) plotted the height
                    of parents against the height of their adult children, he noticed the tendency for tall (or short)
                    parents to have tall (or short) children, but the children were not as tall (or short), on average,
                    as their parents. So while there was correlation between the height of children and their parents,
                    children also balanced out the extreme characteristics of the parents. Galton called this phenomenon
                    “regression toward mediocrity”.
                </p>
                <p>
                    Looking at Galton's data we can see how the average heights of adult
                    children tend to “regress” to the mean of the population. The 45° Line suggests a $1:1$ association
                    in the height of children and parents, in contrast the Regression Line is flatter reflecting the
                    regress of extremes.</p>
                <figure class="flex flex-col justify-center items-center">
                    <div class="w-[80%] hover:scale-125">
                        <img src="img/scatter_reg/galton1.png" alt="Correlation coefficients">
                    </div>
                    <figcaption
                        class="mx-5 mt-10 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">
                        The regression line indicates a weaker than $1:1$ relationship between the average
                        height of parents and their offspring.
                    </figcaption>
                </figure>

                <p>The graph would also look similar today (100 years later), even if people had become taller due
                    to improved health conditions, but the distribution of observations around the mean would remain
                    proportional. Otherwise, if children of tall parents became taller and children of short parents
                    became shorter on average, the spread of observations would become wider and wider, and the
                    world would become a place of dwarfs and giants.</p>
                <p>From Galton's discovery we have learned that: <br />
                    1) Repeated measurements made on the same subject show random fluctuations around a true
                    mean. <br />
                    2) Extreme observations are likely to be followed by less extreme ones. (On average,
                    extremes do not survive)</p>
                <p>From a simple scatter plot alone we can read many things. We see the range of values, the spread of
                    observations, the single and joint mean of the variables, and maybe even some characteristics of the
                    relationship between them like direction, strength (tight or dispersed data points), and degree
                    (linear, non-linear).</p>
                <p>Let us quickly determine these things quickly to understand how the regression line is
                    derived from them. </p>
            </section>

            <!-- spread  -->
            <section>
                <h2 id="spread">Spread</h2>
                <p>Spread is a measure of variability of the observations around the center, usually the arithmetic
                    mean.</p>
                <p>One such measure of spread is the <strong>range</strong>, which is simply the largest measurement
                    minus the smallest, and while it gives us an idea of the size of the numbers we're talking
                    about, it doesn't tell us what's going on in between those boundaries. </p>
                <p>The sample <strong>variance</strong> is more useful, it tells us about the deviations of the
                    observations from the mean $(x_i -\bar{x})$. To compute the sample variance, the deviations are
                    squared, summed, and divided by $(n-1)$, large deviations are emphasized by squaring while
                    negative numbers are removed. Squaring has the additional advantage that negative and positive
                    deviations do not cancel each other out. <br />
                    $$s = \frac{\sum_{i=1}^{n}(x_i - \bar{x})}{n-1}$$<br />
                    The variance now tells us how the data is spread out within the limits set by the range. Of two
                    samples with the same range and mean, the one with the greater variance consists of data that is
                    more spread out, while the other is relatively more clustered around the mean.</p>
                <p>However, squaring the variances leads to odd units, if we work with $\$$ we get $\$^2$. As a
                    result, one often sees the sample <strong>standard deviation</strong> $s$, which is the square
                    root of the variance. The standard deviation gets rid of the squared units, but inherits the
                    advantages of the variance, namely counting positive and negative deviations equally and
                    emphasizing large deviations. The standard deviation can be thought of as a "typical" distance
                    of the observations $x_i$ from their mean, $\bar{x}$. <br />
                    $$s=\sqrt{s}$$</p>
            </section>

            <!-- averages  -->
            <section>
                <h2 id="averages">Averages</h2>
                <p>I have suggested the (arithmetic) mean as the point around which our data is spread. But there
                    are other "measures of central tendency" to identify the center of a sample. </p>
                <p>For example, the <strong>mode</strong>, which is the most frequent value, or the
                    <strong>median</strong>, which is the middle number in a set of ordered (ascending or
                    descending) values, and divides the data set in half, separating the lowest 50% from the highest
                    50% of values.
                </p>
                <p>The median is considered a <strong>robust</strong> statistic because it is only concerned with
                    the values in the middle of the distribution. This means that extreme values do not affect the
                    median.</p>
                <p>However, the arithmetic <strong>mean</strong> is the most commonly used measure of central
                    tendency. It is not robust because it considers all the values in a sample. This makes the mean
                    <strong>sensitive to outliers</strong>, but also makes it a more comprehensive statistic.
                    Distributions with some extreme values are said to be <strong>skewed</strong> to the right/left
                    or to have a right/left tail. For these distributions a robust measure is more appropriate, but
                    for symmetric distributions the mean has some advantages.
                </p>
                <p><strong>Noteworthy advantages of the arithmetic mean</strong>:</p>
                <ul>
                    <li>The sum of the deviations of each data point from the mean is always zero. In other words,
                        the mean acts as a balancing point for the data distribution, with positive and negative
                        deviations canceling each other out.</li>
                    <li>The balancing feature of the arithmetic mean also minimizes the sum of the squared deviations:
                        $\sum (x_i - \bar{x})^2$. </li>
                </ul>
                <p>Minimizing deviations is at the heart of regression analysis and we’ll see why that is in the
                    following.</p>
            </section>

            <!-- point & graph of averages  -->
            <section>
                <h2 id="goa">Point &amp; Graph of Averages</h2>
                <p>In regression we’re interested in the joint behavior of two and more variables. In the case of
                    two variables we want to know how the explanatory variable $x$ describes the response $y$, or at
                    least we try to find out if there is a meaningful relationship between them. </p>
                <p>But even without a model we can construct a fine graphical description of the relationship and
                    even use it to make predictions. </p>
                <p>The roughest estimate of the joint behavior of both variables can be found in the <strong>point
                        of averages</strong> ($\bar{x}, \bar{y}$). This would lead to a statement like "Parents with
                    an average height of 67 inches have children with an average height of 66.5 inches". Not very
                    expressive, but if we want to be more precise, we could look at a certain section of the sample,
                    say parents between 62 and 64 inches and ask about the height of their children. This will give
                    us a <strong>group mean</strong> for the average height of children with parents in that range:
                    $\bar{y}_{62-64}$.</p>
                <p>Now we can slice the sample across the entire range and calculate the different group means
                    $\bar{y}_i$. Connecting these group means gives us a reasonable precursor to the regression
                    line, the <strong>graph of averages</strong>. This graph can be tweaked to capture more or less
                    variance in the data simply by making the groups narrower or wider and calculating the
                    respective group means. Try it out in the interactive plot below.</p>


                <!-- graph of averages  -->
                <figure class="flex flex-col justify-center items-center">
                    <div>
                        <iframe src="plots/graph_of_avg.html" title="Simple Scatter Plot" height="500" width="700"
                            scrolling="no" allowfullscreen loading="lazy" seamless></iframe>
                    </div>
                    <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">
                        More and narrower groups depict more of the variability in the data while wider groups are less
                        affected by the samples peculiarities.
                    </figcaption>
                </figure>

                <p>The ability to reproduce more or less of the variance found in the data is an important decision
                    when building a statistical model, because you want the model to behave realistically in
                    response to the inputs, but at the same time the outputs of your model should not reflect all
                    the peculiarities of the sample data. In machine learning, this is called
                    <strong>generalization</strong>, and the decision between reproducing more or less of the
                    information used to build the model involves a <strong>bias-variance tradeoff</strong>.
                </p>
                <p>While the graph of averages results in a fine linear description of the data, it remains a series
                    of dots connected by a line. A functional description of the graph of averages, however, would
                    require piecemeal linear interpolation along the $x$-segments of the graph. </p>
                <p>The classical regression model, however, is a <strong>parametric model</strong>&nbsp;<input
                        type="checkbox" id="cb1" /><label for="cb1"><sup></sup></label><span><br><br>Parametric models
                        are described by
                        a finite number of parameters and a set of assumptions about the data. As a result, they require
                        less data and less time to build a reasonably accurate model, they are easier to understand, and
                        the results are interpretable, a property not to be underestimated. But this efficiency comes at
                        the cost of some limitations. While finite parameters and a certain set of assumptions simplify
                        the problem, parametric models struggle with complex patterns and nonlinearity. Non-parametric
                        methods, on the other hand, make no explicit assumptions about the functional form of the
                        relationship. Instead, they rely on the data itself to determine the structure of the
                        relationship of the variables.<br><br></span>, that is, a
                    model based on a finite number of parameters. </p>
            </section>

            <!-- sd line  -->
            <section>
                <h2 id="sdline">Standard Deviation Line</h2>
                <p>Now we have the pieces to construct a first functional description of the data. With the point of
                    averages, indicating the center of the data points and the standard deviations $s_{x}$ and $s_y$
                    of both dimensions, we’re able to construct a linear relationship between the two variables,
                    that is specified by a few parameters. </p>
                <p>The <strong>Standard Deviation Line</strong> (SD-Line) originates from the point of averages and
                    extends in the direction described by the vector between the standard deviations. Suppose we
                    have a $s_{x}=1$ and $s_y=2$, then the SD-Line starts at ($\bar{x}, \bar{y}$) and has a slope of
                    $\frac{s_y}{s_x}=2$, resulting in a slanted line, showing a positive relationship. </p>

                <figure class="flex justify-center items-center">
                    <div>
                        <img src="img/scatter_reg/sd_new.png" alt="SD Line">
                    </div>
                    <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

                    </figcaption>
                </figure>

                <p>But there’s a catch. The SD-Line would always show a positive relationship because the standard
                    deviations (as the variance) are always positive. In calculating the standard deviations, we
                    have lost the information that indicates the direction of the deviations. </p>
                <p>Furthermore the standard deviations represent the total variability of a single variable
                    calculated independently from each other. The slope of the SD-Line simply contrasts the two
                    aggregate statistics $s_{x}$ and $s_y$.</p>
                <p>The <strong>correlation coefficient</strong> $r$, corrects the above omissions. The coefficient
                    takes into account the pairwise behavior of the variables at an observation level $(x_i, y_i)$
                    and reintroduces the direction of the relationships.</p>
            </section>

            <!-- correlation  -->
            <section>
                <h2 id="correlation">Pearson's Correlation</h2>
                <p>Pearson's correlation coefficient, assists the SD-Line with a direction of the relationship and
                    it does so by examining the joint behavior of the variables at the level of individual data
                    points. </p>
                <p>The correlation coefficient is defined as:<br />
                    $$r = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i -
                    \bar{x})^{2} \sum_{i=1}^{n}(y_i - \bar{y})^{2}}} $$<br />
                    You can ignore the denominator, as it is simply a standardizing factor and focus on the
                    numerator. In the numerator the deviations in both variables are multiplied, measuring the
                    strength and direction of the joint behavior for each point. </p>
                <p>Let's say we have an observation ($x_{i}, y_i$) that is to the left ($x_i&lt;\bar{x}$) and above the
                    point of averages ($y_i>\bar{y}$). We would be multiplying a negative and a positive value $(x_i
                    - \bar{x})(y_i - \bar{y})$ to get a negative value. Doing this for each observation results in a
                    number between $[-1, +1]$ thanks to the denominator. </p>


                <figure class="flex justify-center items-center">
                    <div>
                        <img src="img/scatter_reg/correlation_quadrants.png" alt="Correlation quadrants">
                    </div>
                    <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

                    </figcaption>
                </figure>

                <p>The sign of the coefficient indicates where the majority of the observations lie from the point
                    of the averages. </p>
                <p>Observations in the lower-left and upper-right quadrants, result in positive terms in the
                    numerator. Conversely, observations in the upper-left and lower-right quadrants result in
                    negative terms. Overall the remaining sign tells us the general direction of the cloud of
                    observations: positive if more observations were in the lower-left and upper-right quadrants,
                    negative when otherwise. The standardized magnitude of the value indicates the strength of the
                    linear relationship between $y$ and $x$ or how tightly or loosely the sample points are
                    clustered. </p>
                <p>Much could be said about the Pearson's correlation coefficient $r$ but I will summarize the most
                    important points about it.</p>
                <ul>
                    <li>$r$ contains information about the direction (sign) and strength (value) of a linear
                        relationship between <strong>two</strong> variables.</li>
                    <li>A $r\approx1$ indicates a tightly clustered positive, and $r\approx-1$ indicates a tightly
                        clustered negative linear relationship of sample points. And $r\approx0$ means that we
                        either we have a random cluster of sample points or a non-linear relationship. </li>
                    <li>For non-linear relationships or straight horizontal / vertical aligned sample points the
                        coefficient fails and is around zero. See in the graph below.</li>
                    <li>Therefore, a low correlation coefficient should not be confused with independence of the
                        variables. To be on the safe side plot your data and make sure it is monotonic (steadily
                        increasing or decreasing), otherwise, the correlation coefficient may not be meaningful.
                    </li>
                    <figure class="flex justify-center items-center">
                        <div>
                            <img src="img/scatter_reg/corrcoeff.svg" alt="Correlation coefficients">
                        </div>
                        <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

                        </figcaption>
                    </figure>
                    <li><strong>Correlation is not causation</strong>. Correlation may be due to pure chance alone
                        or to a third, hidden factor (also known as a confounder) that independently affects the
                        observed variables. </li>
                    <li>The formula for correlation coefficients formula can also be written as: $$r=\frac{s_{x
                        y}}{s_xs_{y}}$$where $s_{x y}=\frac{1}{n-1} \sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})$ is
                        the <strong>sample covariance</strong> that measures the joint variability of $x$ and $y$,
                        while $s_x$ and $s_{y}$ are the respective standard deviations that normalize the
                        covariance. The correlation coefficient can thus be thought of as a normalized version of
                        the covariance.</li>
                    <li>The correlation coefﬁcient is a unitless number. It is not affected by:<ul>
                            <li>Interchanging the two variables.</li>
                            <li><u>Changes of scale</u>: Adding the same number or multiplying all its values by the
                                same positive number only changes the scale of the variable and does not affect $r$.
                            </li>
                        </ul>
                    </li>
                </ul>
                <p>Most importantly for us however is that the correlation coefficient provides the pieces that our
                    SD-Line was missing to become a proper regression model. While the slope of the SD-Line
                    ($\frac{s_y}{s_{x}}$) has no sense of direction and does not provide enough information about
                    the joint behavior of the variables, the correlation coefficient does. </p>




            </section>

            <!-- regression method  -->
            <section>
                <h2 id="regression_method">Regression Method</h2>
                <p>By adjusting the slope of the SD Line with the correlation coefficient we obtain the correct
                    slope of the regression line.<br />
                    $$r*\frac{s_y}{s_{x}}$$<br />
                    A line with this adjusted slope changes only by a certain percentage $(r)$ of the 'typical'
                    deviations ($\frac{s_y}{s_{x}}$) from the point of averages. And the sign of $r$ remedies the
                    stubborn positiveness of the the SD-Line. In the plot below you can adjust the correlation
                    coefficient to see what it does to the SD-Line.</p>
                <p>Combining the slope of the SD-Line and the correlation coefficient is called the regression
                    method. A line anchored in the point of averages $(\bar{x}, \bar{y})$ with a slope described by
                    $r*\frac{s_y}{s_{x}}$ is a full-fledged regression line, but not quite what I promised, namely a
                    formula with an intercept, slope and error term.</p>
                <p>You can play with the slider in the plot below to see what the correlation coefficient does to the
                    SD-Line.</p>

                <!-- correlation coefficient interaction  -->
                <figure class="flex flex-col justify-center items-center object-contain">
                    <div>
                        <iframe src="./plots/correlation_coeff.html" title="correlation interact" width="700"
                            height="500" scrolling="no" allowfullscreen loading="lazy" seamless></iframe>
                    </div>
                    <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">
                        Play with the slider to see how the correlation coefficient $r$ adjusts the slope of the
                        SD-Line. The shown line has a: $\text{slope}= r \times \frac{s_y}{s_x}$. If $r=1$ the line
                        equals the SD-Line, by moving the slider you tune down the rough assumption of the SD-Line
                        &ndash;&nbsp;namely that one step in $s_x$ corresponds to one step in $s_y$&nbsp;&ndash; to be a
                        fraction of $\frac{s_y}{s_x}$. An $r=0$ would suggest that $x$ contributes no information for
                        the prediction of $y$. The 'best fit' line has $r=0.55$. If the observations were mirrored
                        &ndash;&nbsp;going from top left to bottom right&nbsp;&ndash;the correct $r$ would be found in
                        the negative.
                    </figcaption>
                </figure>

                <p>I have introduced you to the regression method, because it allowed me to explain some integral
                    concepts along the way and hopefully gave you an understanding of what regression is all about.
                    Anyway, now it’s time to visit the the more common method of deriving at a regression model with
                    the well-known formula, the method of least squares. </p>
                <p>With the method of least squares we will take a different approach but the result will be the
                    same as the one from the regression method. The rearrangement of $r*\frac{s_y}{s_{x}}$ into the
                    slope $\beta_1$ and the intercept $\beta_0$ will be provided below. </p>
                </div>

            </section>

            <!-- OLS  -->
            <section>
                <h2 id="OLS">Method of Least Squares</h2>
                <p>This method approaches the problem directly by minimizing the errors $(y_i -\hat{y}_i)$. The method
                    is best
                    illustrated graphically. In the figure below we see the same dataset twice with different
                    straight lines placed between the observations. The goal is to find a line that passes through
                    most of the sample points $i$ or at least minimizes the distance between the points (true label
                    $y_i)$ and the estimates $\hat{y}_i$ represented by the line. You may find many lines that will
                    reduce the <em>sum of errors</em> to zero: $\sum(y_i -\hat{y}_i)=0$, due to the fact that
                    positive and negative errors cancel each other out. But it can be shown that there is only one
                    line for which the <em>sum of squared errors</em> $\sum(y_i -\hat{y}_i)^2$ is a minimum. </p>
                <p>The smallest possible sum of squared errors, <strong>$SSE$</strong> for short, would be zero,
                    since there can be no negative value after squaring. Data with a $SSE=0$ were perfectly aligned
                    on a straight line, a deterministic relationship without aberrations, which is obviously
                    not a realistic scenario for natural data. </p>

                <figure class="flex justify-center items-center">
                    <div class="overflow-hidden transition-transform transform hover:scale-150 border-transparent">
                        <img src="img/scatter_reg/SSE.png" alt="Errors of prediction"
                            style="object-fit: contain; width: 100%; height: auto;">
                    </div>
                    <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

                    </figcaption>
                </figure>

                <p>We're left with $SSE&gt;0$, which we want to minimize. We can describe this mathematically as an
                    optimization problem, which allows us to use derivatives to determine optimal parameters for the
                    regression formula. </p>
                <p>Here is the regression formula again, this time with little hats to indicate that the symbols
                    below them are estimates of their unknown population parameters (population parameters do not
                    wear hats).<br />
                    $$\hat{y}=\hat\beta_0+\hat\beta_{1}x$$<br />
                    All of the estimates are unknown to us at this stage, what we have are the data, $x$ and $y$ and
                    what we can infer from them, such as the means $(\bar{x}, \bar{y})$ and standard deviations
                    $(s_x, s_y)$. We can formulate the optimization problem in a way that we derive the unknown
                    parameters, by minimizing and rearranging the symbols. </p>
                <p>Here is a quick summary of the math:</p>
                <p>The goal is to minimize the sum of squared errors: $\sum(y_{i} - \hat{y_i})^2$, the
                    <strong>objective function</strong> to minimize then is: $\sum \left[y_i - (\beta_0 + \beta_1
                    x_i)\right]^2$.
                </p>
                <p>A typical optimization notation is $Q$ for the objective function, with the parameter estimates
                    to be minimized in parentheses. <br />
                    $$\text{Find }\min_{\beta_{0}, \,\beta_1} Q(\beta_{0}, \,\beta_1), \quad\quad \text{for }
                    Q(\beta_{0},\,\beta_1) = \sum \left[y_i - (\beta_0 + \beta_1 x_i)\right]^2$$<br />
                    Minimizing the objective function for the intercept $\beta_0$ is equivalent to finding the best
                    vertical position of the line.<br />
                    $$\frac{\partial Q(\beta_{0}, \beta_1)}{\beta_{0}}\quad \rightarrow \quad
                    \hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$$<br />
                    Minimizing the objective function for $\beta_1$​ ensures that the regression line has the
                    optimal slope to best capture or relationship between the variables.<br />
                    $$\frac{\partial Q(\beta_{0}, \beta_1)}{\beta_{1}}\quad \rightarrow \quad \hat{\beta_1}=
                    \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^{2}} = \frac{s_{x y}}{s_x^2}$$</p>
                <p>$\hat{\beta_1}$ is now defined by terms we already know. <br />
                    $\hat\beta_{0}$ is found by implanting $\hat{\beta_1}$ in the first derivative
                    $\hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$. </p>
                <p>$\hat{\beta_0}, \hat{\beta_1}$ are called the <strong>least squares estimates</strong> of the
                    population parameters $\beta_0$ and $\beta_1$, and the prediction equation
                    $\hat{y}=\hat\beta_0+\hat\beta_{1}x$ is called the <strong>least squares line</strong> or line
                    of best fit. </p>
                <p>The least squares line $\hat{y}=\hat\beta_0+\hat\beta_{1}x$ has the following two properties:</p>
                <ol>
                    <li>The sum of the errors equals 0 , i.e., mean error = 0.<br />
                        $$\sum ε_i =0 \quad \&amp; \quad \bar ε =0$$</li>
                    <li>The $SSE$ is smaller than for any other straight-line model, i.e.,
                        the error variance is minimal.</li>
                </ol>
                <p>The first point explains why we don’t see the error term $ε$ in the least squares line equation,
                    while there is still uncertainty in the predictions the error is implicitly accounted for in the
                    estimated parameters.</p>
                <p>I have said before that this slope of $\hat\beta_1$ is the same as the one found out by combining
                    the slope of the SD-Line and the correlation coefficient. Here's how they are related in
                    symbols:<br />
                    $$\hat\beta_1=r \frac{s_y}{s_{x}}=\frac{s_{x y}}{s_{x}\thinspace s_{y}}\,\frac{s_y}{s_{x}}
                    =\frac{s_{x y}}{s_{x}^2}$$<br />
                    where $r=\frac{s_{x y}}{s_xs_{y}}$ is the correlation coefficient, $s_x$ and $s_y$ are the
                    sample standard deviations, $s_{xy}$ the sample covariance and $s_x^2$ is the sample variance of
                    $x$. </p>
                <p>How do we now find the intersection of our line with the y-axis? Looking at the formula we
                    arrived at, we can understand it as walking from the point of averages $(\bar{x}, \bar{y})$
                    along the slope for the length of one $\bar{x}$ towards the y-axis. We will end up somewhere
                    above or beyond $\bar{y}$, depending on the sign of $\beta_1$, as long as our slope is not zero.
                    <br />
                    $$\hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$$<br />
                    $\hat\beta_0$ represents the predicted value of $y$ when $x = 0$. Note, however, that this value
                    is meaningless if $x = 0$ is nonsensical or outside the range of the sample data.
                </p>

                <figure class="flex justify-center items-center">
                    <div>
                        <img src="img/scatter_reg/intercept.png" alt="Intercept" style="width: 700px; height: 500px;">
                    </div>
                    <figcaption class="m-5 font-sm tracking-tight leading-2 text-neutral-600 dark:text-neutral-400">

                    </figcaption>
                </figure>
                <p>The least square regression model: $\hat{y}=\hat\beta_0+\hat\beta_{1}x$ embodies Galton’s
                    discoveries, that children tend to be taller when their parents are tall, and that observations
                    tend to return to their mean. Galton’s findings are reflected in the estimated parameters as the
                    parameters where constructed by using the point of averages as a reference and describing the
                    deviations of the observations from this point. </p>
            </section>

            <!-- Interpretation -->
            <section>
                <h2 id="interpretation">Interpretation</h2>
                <p>Interpreting the estimated parameters depends on the units of our variables. If we are inferring
                    weight (pounds) from height (inches), we would say for every one-inch increase in height our
                    model suggests a weight increases by $\beta_1$ pounds. </p>

                <div>
                    <img src="img/scatter_reg/weight-height_intercept.png" alt="Interpretation Intercept"
                        style="max-width:300px; float:right; margin:16px 0 4px 20px;">
                </div>
                <div>
                    <p>The intercept $\beta_0$ is more obscure. It represents the value of the response variable
                        when the explanatory variable is zero. In the height and weight example, $\beta_0$
                        represents the weight of someone who is zero inches tall. The regression equation in the
                        graph on the right is $\hat{y}=-106+3.43x$, from the formula alone one would say "a person
                        of zero inches has a weight of $-106$ pounds", a very peculiar person indeed. Obviously, the
                        intercept makes no sense if our explanatory variable is unreasonable.</p>
                    <p>When we talked about predictions in the terminology chapter, I advised against straying too
                        far from the available data to make predictions outside of the range of values you actually
                        observed. Trying to find the weight of a person who is zero inches tall should explain why
                        this it is not a good idea. The intercept is mostly a mathematical tool for drawing a
                        line. Estimates of $\hat{y}$ are most plausible around the center of your observations,
                        beyond the range of observed values the meaningfulness of the estimates diminishes quickly.
                    </p>
                </div>
                <p>Looking again at the concrete example of $\hat{y}=-106+3.43x$, we can say that with each
                    additional inch of height, a person's weight increases by $3.43$ pounds, on average. As long as
                    we stay close to our observed range of values for the explanatory variable, in this case between
                    $63.43$ and $73.90$ inches.</p>
            </section>

            <!-- summary  -->
            <section>
                <h2 id="summary">Summary</h2>
                <p>We have all our pieces together for a regression model that is ready to predict an event by
                    providing the model with an input. Let’s recapture the essentials. </p>
                <ul>
                    <li>A simple linear regression quantifies the effect of an explanatory variable on a response
                        variable, assuming a linear relationship between them.</li>
                    <li>The problem can be stated as an optimization problem, with the goal of minimizing the sum of
                        squared errors $(SSE)$ between true labels and estimates. $$\min_{\beta_{0}, \,\beta_1} \,(SSE)
                        =
                        \min_{\beta_{0}, \,\beta_1} \left\{\sum (y_{i} - \hat{y}_i)^2 \right\} = \min_{\beta_{0},
                        \,\beta_1} \left\{\sum[y_i-(\hat\beta_0+\hat\beta_{1}x_i)]^2\right\}$$
                    </li>
                    <li>The resulting parameter estimates: $\hat\beta_{0}, \hat\beta_{1}$ are composed of sample
                        statistics like the means $(\bar{x}, \bar{y})$, the standard deviations $(s_x, s_{y})$ and
                        the joint variation summarized in the sample covariance $(s_{x y})$.<br />
                        $$\hat{\beta_1}= \frac{s_{x y}}{s_x^2}$$<br />
                        $$\hat\beta_{0}=\bar{y}-\hat{\beta_1}\bar{x}$$</li>
                    <li>A line is found by examining the the variability of the data points around their means,
                        where the point of averages $(\bar{x}, \bar{y})$ serves as a reference for the examination.
                    </li>
                    <li>The final outcome is a simple (two variable relationship) linear regression model concretely
                        implemented by the least square line: $$\hat{y}=\hat\beta_0+\hat\beta_{1}x$$</li>
                </ul>
            </section>

            <!-- sources  -->
            <section class="tracking-tight leading-7">
                <h2 id="sources">Sources</h2>
                <p>McClave, J. T., Benson, P. G., &amp; Sincich, T. (2018). <em>Statistics for business and
                        economics</em> (Thirteenth edition, global edition). Pearson.</p>
                <p>Freedman, D., Pisani, R., &amp; Purves, R. (2007). <em>Statistics</em> (International student
                    ed., 4. ed). Norton.</p>
                <p>Witte, R. S., &amp; Witte, J. S. (2017). <em>Statistics</em> (Eleventh edition). Wiley.</p>

            </section>

        </article>
    </main>

    <!-- footer  -->
    <hr class="mx-20 mt-10 border-gray-200 dark:border-gray-700">
    
    <footer id="footer" class="bg-white dark:bg-gray-900">
        <div class="container px-6 py-4 mx-auto md:mx-20 md:px-0">
            <div class="grid grid-cols-1 gap-4 justify-between mt-5 md:grid-cols-6">

                <!-- Logo and Name  -->
                <div class="col-span-2 hidden sm:block">
                    <div class="flex justify-between items-center">
                        <div class="flex gap-3 items-center">
                            <a>
                                <img id="logo2" class="h-16 fill-current dark:filter-white" src="./img/logo/logo5.svg"
                                    alt="">
                            </a>
                            <div class="">
                                <p
                                    class="pb-3 text-lg font-merri font-semibold tracking-widest uppercase rounded-lg focus:outline-none focus:shadow-outline">
                                    Marco</p>
                                <p
                                    class="text-lg font-merri font-semibold tracking-widest uppercase rounded-lg focus:outline-none focus:shadow-outline">
                                    Zausch</p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="col-span-4 justify-between md:flex">
                    <!-- CONTACT ME  -->
                    <div class="py-3 md:py-0">
                        <div class="text-sm uppercase text-gray-500 dark:text-gray-400 font-medium">Contact Me</div>
                        <a class="flex items-center gap-3 my-3 hover:text-fuchsia-600 dark:hover:text-sky-300"
                            href="mailto:marcozausch@posteo.de">
                            marcozausch@posteo.de</a>
                    </div>

                    <!-- WORK  -->
                    <div class="py-3 md:py-0">
                        <div class="text-sm uppercase text-gray-500 dark:text-gray-400 font-medium">Work</div>

                        <p class="my-3 flex items-center gap-3">
                            <a href="https://www.linkedin.com/in/marcozausch" aria-label="LinkedIn">
                                <span
                                    class="icon-[fa6-brands--linkedin] fill-current w-5 h-5 text-gray-500 dark:text-gray-300 hover:text-fuchsia-600 dark:hover:text-sky-300"></span>
                            </a>
                            <a class="block hover:text-fuchsia-600 dark:hover:text-sky-300"
                                href="https://www.linkedin.com/in/marcozausch">Linked In</a>
                        </p>

                        <p class="my-3 flex items-center gap-3">
                            <a href="https://github.com/MaCoZu" aria-label="Github">
                                <span
                                    class="icon-[fa6-brands--github] fill-current w-5 h-5 text-gray-500 dark:text-gray-300 hover:text-fuchsia-600 dark:hover:text-sky-300"></span>
                            </a>
                            <a class="block hover:text-fuchsia-600 dark:hover:text-sky-300"
                                href="https://github.com/MaCoZu">Github
                            </a>
                        </p>
                    </div>

                    <!-- RESSOURCES -->
                    <div class="py-3 md:py-0">
                        <div class="text-sm uppercase text-gray-500 dark:text-gray-400 font-medium">Resources</div>
                        <a class="my-3 block hover:text-fuchsia-600 dark:hover:text-sky-300" href="/#">Statistics </a>
                        <a class="my-3 block hover:text-fuchsia-600 dark:hover:text-sky-300" href="/#">Tutorials </a>
                    </div>

                    <!-- LEGAL  -->
                    <div class="py-3 md:py-0">
                        <div class="text-sm uppercase font-medium text-gray-500 dark:text-gray-400 ">Legal</div>
                        <a class="my-3 block hover:text-fuchsia-600 dark:hover:text-sky-300"
                            href="./impressum.html">Impressum
                        </a>
                        <a class="my-3 block hover:text-fuchsia-600 dark:hover:text-sky-300"
                            href="./privacypolicy.html">Privacy Policy
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </footer>

    <script type="module" src="../main.js"></script>
    <script type="module" src="../script.js"></script>
</body>

</html>